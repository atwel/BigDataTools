{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<p><b><font size=6>Lab 3 - Twitter</font></b></p>\n",
      "<br>\n",
      "Twitter is a fascinating form of media in its own right, but as the platform has grown in size and influence, it has become a wonderful site for exploring questions about the social world. In this lab you will explore Twitter data and produce some descriptive findings. This module can scrape, archive, search and display tweets interactively. Because Twitter does not typically provide access to all tweets, you should always remember that this is a sample and that any inferences you make should come with the caveat that you don't know anything about the sampling procedure that produced the sample.\n",
      "\n",
      "To get started the first thing we need to do is import the core module with the `import twttr` statement below. We'll also import a package for displaying HTML within this notebook using the `from IPython.dispay import HTML` statement."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import twttr\n",
      "from IPython.display import HTML"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "\n",
      "Before you can get started with collecting and analyzing, you'll need to authenticate to the Twitter API. The very first time you run the code in the cell below, you'll need to authenticate using your own Twitter account, so if you don't have one you'll want to register now. \n",
      "\n",
      "When you hit shift+enter to run the code the first time, you'll be taken to the Twitter website to sign in. It will then provide you with a PIN number to insert into the newly visible blue highlighted space in the cell above. Once you've done that, it will save your login data. You'll still need to run the login code everytime you start this notebook, but you won't need to authenticate via Twitter's webpage again.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twttr.login()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "One of the first things you might want to do is to find a user. The command `twttr.viewUser(\"@user_handle\")` does that and reports some of their metadata. It also returns a string that allows you to render the results as HTML using the aptly named `HTML()` function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = twttr.viewUser(\"chrisrock\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HTML(a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That command can be helpful to make sure you have the right person, but you'll probably stick to other, more powerful commands.\n",
      "<br><br>\n",
      "##getUsersTweets( )##\n",
      "One such command is `getUsersTweets( )`.\n",
      "It gets upto 200 of the user's most recent tweets. Here is an example of its use:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "the_tweets = twttr.getUsersTweets(\"chrisrock\", count=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "##index_Tweets()##\n",
      "\n",
      "After you've run the `getUsersTweets()` command, all of the Tweet objects are sitting in the list `the_tweets`. You can preview them in the output window beneath the command after you've run it, but if you want to save them for future analysis, you must index them using the `index_Tweets( )` command. This command creates a searchable database using the Python module **WHOOSH**. We'll cover how to do searchs later in this lab, but first we need to get them into the database so that we have something to search through!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twttr.index_Tweets(the_tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "This command accepts a `batch` argument to expedite retrieving these exact tweets later. To use this functionality, simply provide a unique batch number whenever you run the command. You can then provide this number as a search term later. The default batch number is 1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = twttr.index_Tweets(the_tweets, batch=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##tweetSearch(  )##\n",
      "Searching by users can be a great way to get data, but you'll probably also want to search by terms, hashtags, or even URLs. To do that, you use the `tweetSearch( )` method shown here. Twitter caps the number of tweets it will return at 100 which you might expect is a very small sample of all the possible tweets matching your search term(s). Twitter doesn't tell us much about how they determine the sample, but they do allow us to specify whether we want the most recent tweets, the most popular or a mixture of both. You can specify this with the second argument below; the options are `popular`, `recent`, or `mixed` and the default if you omit this argument is `recent`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "movieTweets = twttr.tweetSearch(\"Top Five\", \"recent\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, to ensure that you can look at and analyze these tweets later, you need to index them using the `index_Tweets( )` command. To suppress the output of the command, we can assign the output to any named object. In the line below, this is done by adding `a =` in front of the command but it could have just as easily been `I_dont_care_about_this_stuff =`. It's up to you what the name is."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "a = twttr.index_Tweets(movieTweets, batch=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "## Search the index ##\n",
      "<br>\n",
      "Once you've put some tweets into the index, you can search for ones that match the criteria you specify. You use the `search( )` command to do this. `search( )` accepts arguments for any combination of fields stored in the index. For this particular index, those terms are as follows:\n",
      "\n",
      "  + `id`        - The unique integer that identifies this tweet\n",
      "  + `owner`     - The `user name` of the original poster\n",
      "  + `batch`     - The batch number that you've been specifying. (default=1)\n",
      "  + `text`      - The content of the tweet itself\n",
      "  + `posted`    - The time the tweet was posted (GMT +0:00)\n",
      "  + `isRT`      - (True/False) Whether the tweet was a retweet\n",
      "  + `timesRT`   - The number of times the tweet was retweeted\n",
      "  + `timesFav`  - The number of times the tweet was favorited\n",
      "  + `hashtags`  - All the hashtags appearing in the tweet\n",
      "  + `mentions`  - All the mentions in the tweet\n",
      "  + `hasPic`    - (True/False) Whether the tweet had a picture \n",
      "  \n",
      "These arguments are entered all together in a single string, a series of character within double quotation marks. We'll go over the details of the syntax shortly.\n",
      "  \n",
      "In addition to these index arguments, you can search by a period of the day using the regular `time_slice` argument. The argument looks like this: `[\"09:00\":\"17:00\"]`. The period starts at the time on the left and ends on the time on the right and it is important that there are two numbers on each side of the colon. If the starting time is later than the ending time, it searches past midnight and into the next day.\n",
      "\n",
      "You can also limit the number of tweets returned using the regular `limit` argument. The default is 100.\n",
      "\n",
      "Finally, you can save your search results for viewing as an HTML page later using the `saveAs` argument. The default name is `search`\n",
      "<br><br>\n",
      "Below are some examples of proper syntax. All of these variants can be combined into a single command but are in separate commands here for visual clarity.\n",
      "\n",
      "Here is an example looking at the hashtags and posting times. We'll address the data the command returns later in the lab"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results, data, display, tweet_list = twttr.search(\"hashtags:'topfivemovie' posted:'Nov 22 to Dec 12'\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note the use of multiple quotation marks. **Any argument in that appears in the bulleted list above must appear within the double quotation marks**. There are also single quotation marks for the values that follow the key terms. Also note that there is no comma between key terms"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results, data, display, tweet_list = twttr.search(\"hashtags:'topfivemovie' isRT:'True'\", saveAs=\"chrisrock\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The *timeRT* and *timesFav* key terms allow you to find the most popular tweets (or the least or in between).  Whether there is an important difference between favoriting or retweeting from the prospective of users is not clear to me, but you have both at your disposal. <br><br>\n",
      "Here are additional examples of search queries.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results, data, display, tweet_list = twttr.search(\"hashtags:'topfivemovie' mentions:'ginoseast'\", limit=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This next one gets up to 100 tweets made between the hours of 10pm and 3am and save the results in a file named LateNight_tweets.html in the folder this notebook is in."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results, data, display, tweet_list = twttr.search(\"batch:1 posted:'Nov 22 to Dec 12'\", time_slice=[\"22:00\",\"03:00\"], saveAs=\"LateNight_tweets\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The query below searches the text of the tweets for the variations of a phrase."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results, data, display, tweet_list = twttr.search(\"'Top Five' OR 'TopFive' OR 'top five' OR '#topfive'\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This next one searches for tweets that are either tagged with a search term or mentions a user's offical Twitter account."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results, data, display, tweet_list = twttr.search(\"hashtags:'topfivemovie' OR mentions:'chrisrock'\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Remember#### \n",
      "All of these terms can be used simultaneously. Once you get the hang of it, you can do some very powerful sorting. However the syntax is very finicky so if you get 0 results and you're expecting some, make sure there aren't extra commas or spaces and that the quotation marks are always in pairs."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####What search( ) does###\n",
      "Running the search command does a lot of things behind the scenes. It packages information up for you in the terms that appear on the left hand side of the equals sign. \n",
      "The first term is named `results` but you can name it whatever you'd like by replacing `results` with your term before running the command. (You can do this for all terms by the way.) Results is an object that can be used to display the averages of the individual LIWC values for the whole group of Tweets. \n",
      "`data` is the raw information that can be used to find differences between LIWC averages from different search results, a topic covers below. \n",
      "`display` can be used to view the individual tweets in the search results in a HTML table. \n",
      "Finally, `tweet_list` is a list of the Tweet IDs for the tweets in the search results.\n",
      "<br>\n",
      "<br>\n",
      "The other thing that the command does is produce an HTML file with the Tweets represented as circles proportional to the number of times it was retweeted. An example of the link can be found [here.](files/tools/tweetpack.html) Your search results show up in the folder this notebook is in under the name you specified with the `saveAs` argument."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br><br>\n",
      "##Inspecting Retweets##\n",
      "What you've been collecting and looking at up until now are just a handful of tweets but we really want to look at what happens to the tweet after the original poster submitted it. Because of Twitter's rate-limiting policies, this can be a time consuming task and accordingly the indexing command doesn't collect those data immediately. Rather, you need to tell the index which ones to collect using the `inspectRetweets( )` command. The argument is just a list of tweet ids, most likely gotten from the `search( )` command above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twttr.inspectRetweets(tweet_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This command can take a long time to run and the amount of time depends on two things. The first is the number of tweets; the more you have in the list argument, the longer it will take. The second is whether or not the tweets are from the same person. The code looks up the poster's friends and followers to determine relationship the person has to the retweeters and saves the data. This process is slow (in the world of computers, that is) so if you are looking at posts from a bunch a people, it can take a long time. So if you have a stack of tweets from Oprah, it might take a couple of minutes. But if you have the most popular Tweets about the Hunger Games, you could get dinner and it might not be finished.\n",
      "<br><br>\n",
      "###What Inspecting Retweets Does###\n",
      "Inspecting the retweets looks at the who retweeted the original, when they did and the nature of their relationship to the original poster. It also summarizes these data and visualizes the network of the retweets in a webpage reachable by clicking on the tweet in search interface. The circle representing the tweet will turn blue if you've inspected it.  You can then click on it and be taken to the page with more details. \n",
      "\n",
      "<br>\n",
      "This page includes a visualization of the network in which the length of the ties is proportional to the number of hours after the original tweet that the retweet was posted. The nodes are also colored according to the retweeter's relationship with the original poster. An important thing to note about these data is that Twitter limits the information to a random sampling of 100 of the retweets. So while the tweet might have 2000 retweets, you'll see at most 101 nodes in this network. For the purpose of this lab, you may consider this a representative sample and conduct your analyze using these data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Backtracking: Viewing Results ##\n",
      "\n",
      "When you run a search, you can see the titles of the posts that the query found but it is actually spitting back much more information. We can look at those using the data object `results`, `data`, and `display`.\n",
      "<br>\n",
      "While it appears last, let's start with `display` first. It contains all of the raw information from the posting returned by the query. To make it easier to look at, it is packaged up as an HTML file that we can look at by running the command below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HTML(display)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A couple of things to notice about this:\n",
      "\n",
      "1) That is a lot of information in our poor little Notebook. If you'd rather not look at it here, don't run the command and just go find the file named 'search_results.html' in your CL folder. You can even just keep that file open in your browser and refresh it after new searchs. Doing so will make your notebook easier to use and the HTML also renders better there.\n",
      "\n",
      "2) There is big table at the bottom of each post. These contain the textual analysis of the post using the LIWC dictionary. It contains basic statistics like word counts (WC) and the number of pronouns but also the frequency of words related to positive emotions (posemo) and tentativeness (tentat). The full names and categories for the terms are available on the LIWC site [here](http://www.liwc.net/descriptiontable1.php)\n",
      "\n",
      "3) The tables aren't the same for each post. That's because some posted scored zero on the LIWC count and they've been omitted in order to condense the information. \n",
      "\n",
      "4) These tables are really good for exploring patterns worth looking into more, but chances are you aren't going to want to find averages for these tables by hand. Thats what the `results` object does for you.<br>\n",
      "\n",
      "##Viewing Group Statistics##\n",
      "The *results* object contains useful information about that group of search results, namely the average, the standard deviation, the maximum value and the minimum value for each of the LIWC categories. You can display it as HTML using the command below or via the file *search_averages.html*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HTML(results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, the `data` object contains the raw numbers of the results. These can be used to compare different search groups and create smaller tables of more relevant data. To do that, you need to specify two different searches and give the outputs different names as is shown in the next two lines"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results_1, data_1, display_1, tweet_list_1 = twttr.search(\"batch:1\", time_slice=[\"22:00\",\"03:00\"], saveAs=\"latenight\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results_2, data_2, display_2, tweet_list_2 = twttr.search(\"batch:1\", time_slice=[\"03:00\",\"22:00\"], saveAs=\"day\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You then put the two data objects, 'data_1' and 'data_2' into the 'LIWC_differences( )' command to get a table back. You can then display it by passing it to the 'HTML( )' command. The two lines below do it for the searches above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "diffs = twttr.LIWC_differences(data_1, data_2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HTML(diffs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, if you'd like to trim this big table to just the relevant information, run the 'LIWC_differences_subset( )' command with a list of the terms you want. This is a handy thing for producing actual results to be included somewhere. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subset = twttr.LIWC_differences_subset(data_1, data_2, ['WC', 'posemo', 'negemo', 'swear', 'anger', 'affect'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HTML(subset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}